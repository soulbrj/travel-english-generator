# streamlit_app.py
"""
æ—…è¡Œè‹±è¯­è§†é¢‘ç”Ÿæˆå™¨ â€” ç¦»çº¿ä¼˜å…ˆï¼ˆå¹¶æ”¯æŒåœ¨çº¿å›é€€ï¼‰
åŒ…å«ï¼š
- Excel æ•°æ®å¯¼å…¥ä¸éªŒè¯ï¼ˆå¼ºåˆ¶åˆ—åï¼šè‹±è¯­ã€ä¸­æ–‡ã€éŸ³æ ‡ï¼ˆå¯é€‰ï¼‰ï¼‰
- è§†é¢‘æ ·å¼å®šåˆ¶ï¼ˆèƒŒæ™¯ã€æ–‡å­—æ ·å¼ã€æ–‡å­—èƒŒæ™¯æ¿ã€é—´è·ç­‰ï¼‰
- å¤šéŸ³è‰²éŸ³é¢‘ç³»ç»Ÿï¼ˆç¦»çº¿ pyttsx3 ä¼˜å…ˆ / edge-tts å›é€€ï¼‰
- 4 æ®µéŸ³é¢‘é¡ºåºç¼–æ’ä¸æ··åˆ
- è§†é¢‘ç”Ÿæˆï¼ˆPIL æ¸²æŸ“å¸§ + FFmpeg åˆæˆï¼‰
- å®æ—¶é¢„è§ˆä¸ä¸‹è½½
"""
import os
import io
import sys
import shutil
import tempfile
import asyncio
import threading
import time
import math
import traceback
from typing import List, Dict, Tuple, Optional

import streamlit as st
import pandas as pd
import numpy as np
from PIL import Image, ImageDraw, ImageFont, ImageOps
import imageio.v2 as imageio
import subprocess
import base64

# TTS engines
try:
    import pyttsx3
    PYTTSX3_AVAILABLE = True
except Exception:
    PYTTSX3_AVAILABLE = False

try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except Exception:
    EDGE_TTS_AVAILABLE = False

# ------------------------
# Config
# ------------------------
MAX_ROWS_SUGGEST = 50  # å»ºè®®ä¸è¶…è¿‡ 50 è¡Œç”Ÿæˆ
DEFAULT_RESOLUTIONS = {
    "640x360": (640, 360),
    "854x480": (854, 480),
    "1280x720": (1280, 720),
    "1920x1080": (1920, 1080),
}

# Default fonts: try to find system fonts for Chinese and phonetics
def find_font_candidates():
    # Candidate paths - best effort
    cand = []
    if sys.platform.startswith("win"):
        cand += [
            r"C:\Windows\Fonts\arial.ttf",
            r"C:\Windows\Fonts\msyh.ttc",
            r"C:\Windows\Fonts\simhei.ttf",
        ]
    elif sys.platform.startswith("darwin"):
        cand += [
            "/System/Library/Fonts/Supplemental/Arial.ttf",
            "/System/Library/Fonts/Supplemental/STHeiti Medium.ttc",
        ]
    else:
        # linux common
        cand += [
            "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
            "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",
            "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
        ]
    for p in cand:
        if os.path.exists(p):
            return p
    return None

DEFAULT_FONT_PATH = find_font_candidates()

# ------------------------
# Utilities
# ------------------------
def check_ffmpeg() -> bool:
    return shutil.which("ffmpeg") is not None

def ensure_dir(d):
    os.makedirs(d, exist_ok=True)

def safe_remove(path):
    try:
        if os.path.isdir(path):
            shutil.rmtree(path)
        elif os.path.exists(path):
            os.remove(path)
    except Exception:
        pass

# ------------------------
# Excel Data Handling
# ------------------------
REQUIRED_COLUMNS = ["è‹±è¯­", "ä¸­æ–‡", "éŸ³æ ‡"]  # éŸ³æ ‡å¯é€‰ï¼Œä½†åˆ—åå¿…é¡»å­˜åœ¨ï¼ˆå¯ä¸ºç©ºï¼‰

def validate_and_load_excel(uploaded_file) -> Tuple[Optional[pd.DataFrame], List[str]]:
    """
    éªŒè¯ Excel æ–‡ä»¶ï¼Œè¦æ±‚åŒ…å«å¿…é¡»åˆ—åï¼ˆè‹±è¯­ã€ä¸­æ–‡ã€éŸ³æ ‡ï¼‰ï¼›
    è¿”å› (df, errors)
    """
    errors = []
    try:
        df = pd.read_excel(uploaded_file)
    except Exception as e:
        errors.append(f"Excel è§£æå¤±è´¥: {e}")
        return None, errors

    cols = list(df.columns)
    # Normalize columns by stripping spaces
    cols_clean = [str(c).strip() for c in cols]
    df.columns = cols_clean

    # Check for at least è‹±è¯­ and ä¸­æ–‡ columns; éŸ³æ ‡åˆ—å¯å­˜åœ¨æˆ–ä¸å­˜åœ¨
    if "è‹±è¯­" not in df.columns or "ä¸­æ–‡" not in df.columns:
        errors.append("å¿…é¡»åŒ…å«åˆ—åï¼š'è‹±è¯­' å’Œ 'ä¸­æ–‡' (ç²¾ç¡®åŒ¹é…)ã€‚éŸ³æ ‡åˆ—ä¸ºå¯é€‰ï¼Œä½†æ¨èæ·»åŠ  'éŸ³æ ‡' åˆ—ã€‚")
        return None, errors

    # Ensure 'éŸ³æ ‡' column exists; if not, create empty
    if "éŸ³æ ‡" not in df.columns:
        df["éŸ³æ ‡"] = ""

    # Basic format checks: non-empty è‹±è¯­åˆ—
    if df["è‹±è¯­"].isnull().all():
        errors.append("è‹±è¯­åˆ—å…¨éƒ¨ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")
        return None, errors

    # Trim whitespace
    df["è‹±è¯­"] = df["è‹±è¯­"].astype(str).map(lambda s: s.strip())
    df["ä¸­æ–‡"] = df["ä¸­æ–‡"].astype(str).map(lambda s: s.strip())
    df["éŸ³æ ‡"] = df["éŸ³æ ‡"].astype(str).map(lambda s: s.strip())

    # Optional limit enforcement warning
    if len(df) > 500:
        errors.append("è­¦å‘Šï¼šä¸Šä¼ æ–‡ä»¶è¡Œæ•°è¾ƒå¤šï¼ˆ>500ï¼‰ï¼Œå»ºè®®åˆ†æ‰¹ç”Ÿæˆä»¥é™ä½å†…å­˜ä¸æ—¶é—´å¼€é”€ã€‚")

    return df, errors

# ------------------------
# TTS: voice libraries and generation
# ------------------------
# We'll provide a simulated voice library mapping. For edge-tts use official voice names if installed.
# For pyttsx3, available voices depend on system; we will list them when available.

def list_local_voices():
    """Return list of dict {'id','name'} from pyttsx3 if available"""
    out = []
    if not PYTTSX3_AVAILABLE:
        return out
    try:
        engine = pyttsx3.init()
        voices = engine.getProperty("voices")
        for v in voices:
            try:
                out.append({"id": getattr(v, "id", None), "name": getattr(v, "name", str(v))})
            except Exception:
                continue
        try:
            engine.stop()
        except:
            pass
    except Exception:
        pass
    return out

# Predefined voice sets (names are suggestions; availability depends on engine)
EN_MALE_PRESETS = [
    "en-US-GuyNeural", "en-US-BenjaminNeural", "en-GB-RyanNeural", "en-AU-WilliamNeural",
    "en-US-Tom", "en-US-Mark", "en-GB-Oliver", "en-IE-Darragh"
][:8]

EN_FEMALE_PRESETS = [
    "en-US-JennyNeural", "en-US-AriaNeural", "en-GB-SoniaNeural", "en-AU-NatashaNeural",
    "en-US-Jessica", "en-US-Linda", "en-GB-Emma"
][:7]

ZH_PRESETS = [
    "zh-CN-XiaoxiaoNeural", "zh-CN-YunxiNeural", "zh-CN-KangkangNeural", "zh-CN-XiaoyouNeural",
    "zh-CN-YunfeiNeural", "zh-CN-YunjianNeural", "zh-CN-YunxiNeural",
    "zh-TW-HsiaoChenNeural", "zh-TW-YunJheNeural", "zh-CN-XiaohanNeural",
    "zh-CN-XiaoyanNeural", "zh-CN-NannanNeural", "zh-CN-MeiNeural",
    "zh-CN-YatingNeural", "zh-CN-YifeiNeural"
][:15]

# Aggregate voices to present in UI for selection by category
VOICE_LIBRARY = {
    "è‹±æ–‡ç”·å£°": EN_MALE_PRESETS,
    "è‹±æ–‡å¥³å£°": EN_FEMALE_PRESETS,
    "ä¸­æ–‡éŸ³è‰²": ZH_PRESETS
}

# TTS generation functions
def save_pyttsx3_wav(text: str, voice_id: Optional[str], rate: int, out_wav: str) -> bool:
    """Save text to wav using pyttsx3; return True if success"""
    if not PYTTSX3_AVAILABLE:
        return False
    try:
        engine = pyttsx3.init()
        if voice_id:
            try:
                engine.setProperty("voice", voice_id)
            except Exception:
                pass
        engine.setProperty("rate", rate)
        engine.save_to_file(text, out_wav)
        engine.runAndWait()
        try:
            engine.stop()
        except:
            pass
        return os.path.exists(out_wav) and os.path.getsize(out_wav) > 0
    except Exception:
        return False

def wav_to_mp3_ffmpeg(wav_path: str, mp3_path: str) -> bool:
    if not check_ffmpeg():
        return False
    cmd = ["ffmpeg", "-y", "-i", wav_path, "-q:a", "4", "-acodec", "libmp3lame", mp3_path]
    try:
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        return os.path.exists(mp3_path) and os.path.getsize(mp3_path) > 0
    except Exception:
        return False

def generate_offline_mp3(text: str, voice_id: Optional[str], speed: float, out_mp3: str) -> bool:
    """
    Generate mp3 using pyttsx3 (wav -> mp3). speed is multiplier (0.5-2.0)
    """
    fd, tmpwav = tempfile.mkstemp(suffix=".wav")
    os.close(fd)
    rate_wpm = int(200 * speed)
    ok = save_pyttsx3_wav(text, voice_id, rate_wpm, tmpwav)
    if not ok:
        safe_remove(tmpwav)
        return False
    ok2 = wav_to_mp3_ffmpeg(tmpwav, out_mp3)
    safe_remove(tmpwav)
    return ok2

# edge-tts async wrapper for saving mp3
async def _edge_save_async(text: str, voice: str, out_path: str, rate_str: str = "+0%"):
    try:
        communicate = edge_tts.Communicate(text=text, voice=voice, rate=rate_str)
        await communicate.save(out_path)
        return True
    except Exception:
        return False

def generate_edge_mp3(text: str, voice: str, speed: float, out_mp3: str) -> bool:
    if not EDGE_TTS_AVAILABLE:
        return False
    pct = int((speed - 1.0) * 100)
    rate_str = f"{pct:+d}%"
    try:
        # run async function
        return asyncio.run(_edge_save_async(text, voice, out_mp3, rate_str))
    except Exception:
        return False

def generate_tts_segment(text: str, voice_category: str, voice_choice: str, speed: float, engine_pref: str, out_mp3: str) -> bool:
    """
    engine_pref: "ç¦»çº¿ä¼˜å…ˆ" or "åœ¨çº¿ä¼˜å…ˆ"
    voice_category: category from VOICE_LIBRARY keys or "local"
    voice_choice: for local engines may be pyttsx3 id
    """
    # Try offline first if preferred
    if engine_pref == "ç¦»çº¿ä¼˜å…ˆ" and PYTTSX3_AVAILABLE:
        # voice_choice can be id for local voices. For preset edge names, pyttsx3 will likely ignore.
        ok = generate_offline_mp3(text, voice_choice if voice_choice else None, speed, out_mp3)
        if ok:
            return True
    # Try edge if available (voice_choice may be edge name)
    if EDGE_TTS_AVAILABLE:
        # If voice_choice not provided, pick default depending on category
        voice = None
        if voice_choice:
            voice = voice_choice
        else:
            if voice_category in VOICE_LIBRARY and VOICE_LIBRARY[voice_category]:
                voice = VOICE_LIBRARY[voice_category][0]
        if voice:
            ok = generate_edge_mp3(text, voice, speed, out_mp3)
            if ok:
                return True
    # Finally try offline if not tried yet
    if PYTTSX3_AVAILABLE:
        ok = generate_offline_mp3(text, voice_choice if voice_choice else None, speed, out_mp3)
        if ok:
            return True
    return False

# ------------------------
# Rendering: frame generation (PIL)
# ------------------------
def smart_wrap_text(draw: ImageDraw.Draw, text: str, font: ImageFont.FreeTypeFont, max_width: int) -> List[str]:
    """
    Smart wrap that handles mixed English/Chinese: break on spaces for English, or by character for Chinese.
    """
    lines = []
    # If contains spaces, prefer wrapping at spaces
    if " " in text:
        words = text.split(" ")
        cur = ""
        for w in words:
            test = (cur + " " + w).strip()
            bbox = draw.textbbox((0,0), test, font=font)
            wlen = bbox[2] - bbox[0]
            if wlen <= max_width or cur == "":
                cur = test
            else:
                lines.append(cur)
                cur = w
        if cur:
            lines.append(cur)
    else:
        # no spaces: treat as CJK - wrap by characters
        cur = ""
        for ch in text:
            test = cur + ch
            bbox = draw.textbbox((0,0), test, font=font)
            if bbox[2] - bbox[0] <= max_width:
                cur = test
            else:
                lines.append(cur)
                cur = ch
        if cur:
            lines.append(cur)
    return lines

def render_frame_image(
    en_text: str,
    phonetic: str,
    zh_text: str,
    conf: dict,
    size: Tuple[int,int],
    font_paths: dict
) -> Image.Image:
    """
    Render a single frame image with given texts and style config.
    conf: dict includes background (color or image), style settings for each layer,
          text background panel settings, spacing, paddings, etc.
    font_paths: dict {'main': path, 'phonetic': path, 'chinese': path}
    """
    W, H = size
    # Start with background
    if conf.get("bg_mode") == "image" and conf.get("bg_image_obj") is not None:
        # use uploaded image and adaptively fill
        bg_img = conf["bg_image_obj"].convert("RGBA")
        # Resize to fill while keeping aspect ratio (cover)
        bg_w, bg_h = bg_img.size
        ratio = max(W/bg_w, H/bg_h)
        new_w = int(bg_w*ratio)
        new_h = int(bg_h*ratio)
        bg_img = bg_img.resize((new_w, new_h), Image.LANCZOS)
        # crop center
        x1 = (new_w - W)//2
        y1 = (new_h - H)//2
        bg_crop = bg_img.crop((x1, y1, x1+W, y1+H)).convert("RGB")
        base = bg_crop
    else:
        # solid color
        color = conf.get("bg_color", "#FFFFFF")
        base = Image.new("RGB", (W,H), color)
    draw = ImageDraw.Draw(base)

    # Fonts
    def load_font(path, size_px):
        try:
            if path and os.path.exists(path):
                return ImageFont.truetype(path, size_px)
            elif DEFAULT_FONT_PATH:
                return ImageFont.truetype(DEFAULT_FONT_PATH, size_px)
            else:
                return ImageFont.load_default()
        except Exception:
            try:
                return ImageFont.truetype(DEFAULT_FONT_PATH, size_px)
            except Exception:
                return ImageFont.load_default()

    font_main = load_font(font_paths.get("main"), conf.get("english_size", 80))
    font_ph = load_font(font_paths.get("phonetic"), conf.get("phonetic_size", 60))
    font_cn = load_font(font_paths.get("chinese"), conf.get("chinese_size", 70))

    # Compute text area width
    content_width = int(W * conf.get("text_area_width_ratio", 0.9))
    padding = conf.get("text_padding", 20)
    # Wrap lines
    # create a temporary draw for measuring
    tmp_draw = ImageDraw.Draw(Image.new("RGB",(10,10)))
    en_lines = smart_wrap_text(tmp_draw, en_text, font_main, content_width - 2*padding)
    ph_lines = smart_wrap_text(tmp_draw, phonetic, font_ph, content_width - 2*padding) if phonetic else []
    cn_lines = smart_wrap_text(tmp_draw, zh_text, font_cn, content_width - 2*padding)

    # Compute total height
    line_spacing = conf.get("line_spacing", 10)
    en_h = sum([tmp_draw.textbbox((0,0), l, font=font_main)[3] - tmp_draw.textbbox((0,0), l, font=font_main)[1] + line_spacing for l in en_lines])
    ph_h = sum([tmp_draw.textbbox((0,0), l, font=font_ph)[3] - tmp_draw.textbbox((0,0), l, font=font_ph)[1] + line_spacing for l in ph_lines])
    cn_h = sum([tmp_draw.textbbox((0,0), l, font=font_cn)[3] - tmp_draw.textbbox((0,0), l, font=font_cn)[1] + line_spacing for l in cn_lines])

    total_text_h = en_h + ph_h + cn_h + conf.get("english_phonetic_gap", 10) + conf.get("phonetic_cn_gap", 10)

    # Positioning: center vertically
    start_y = (H - total_text_h) // 2

    # Optional text background plate
    if conf.get("text_bg_enable", False):
        plate_w = int(content_width)
        plate_h = int(total_text_h + 2*padding)
        plate_x = (W - plate_w)//2
        plate_y = start_y - padding
        plate_color = conf.get("text_bg_color", "#000000")
        plate_alpha = int(255 * conf.get("text_bg_alpha", 0.5))
        radius = conf.get("text_bg_radius", 20)
        # build rounded rectangle with alpha
        plate = Image.new("RGBA", (plate_w, plate_h), (0,0,0,0))
        plate_draw = ImageDraw.Draw(plate)
        # draw rounded rect
        rect_color = hex_to_rgb(plate_color) + (plate_alpha,)
        round_rect(plate_draw, [0,0,plate_w,plate_h], radius, fill=rect_color)
        base = base.convert("RGBA")
        base.alpha_composite(plate, dest=(plate_x, plate_y))
        base = base.convert("RGB")
        draw = ImageDraw.Draw(base)

    # Draw english lines
    cur_y = start_y
    for line in en_lines:
        bbox = draw.textbbox((0,0), line, font=font_main)
        w = bbox[2] - bbox[0]
        x = (W - w)//2
        draw.text((x, cur_y), line, font=font_main, fill=conf.get("english_color", "#000000"))
        cur_y += bbox[3] - bbox[1] + line_spacing

    cur_y += conf.get("english_phonetic_gap", 10)
    for line in ph_lines:
        bbox = draw.textbbox((0,0), line, font=font_ph)
        w = bbox[2] - bbox[0]
        x = (W - w)//2
        draw.text((x, cur_y), line, font=font_ph, fill=conf.get("phonetic_color", "#666666"))
        cur_y += bbox[3] - bbox[1] + line_spacing

    cur_y += conf.get("phonetic_cn_gap", 10)
    for line in cn_lines:
        bbox = draw.textbbox((0,0), line, font=font_cn)
        w = bbox[2] - bbox[0]
        x = (W - w)//2
        draw.text((x, cur_y), line, font=font_cn, fill=conf.get("chinese_color", "#222222"))
        cur_y += bbox[3] - bbox[1] + line_spacing

    return base

# Helper: rounded rectangle draw
def round_rect(draw: ImageDraw.Draw, box, radius, fill):
    x1,y1,x2,y2 = box
    draw.rounded_rectangle(box, radius=radius, fill=fill)

def hex_to_rgb(hex_color):
    hex_color = hex_color.lstrip("#")
    lv = len(hex_color)
    if lv == 6:
        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
    elif lv == 3:
        return tuple(int(hex_color[i]*2, 16) for i in range(3))
    else:
        return (0,0,0)

# ------------------------
# Video generation pipeline
# ------------------------
def generate_video_from_df(
    df: pd.DataFrame,
    selected_rows: List[int],
    style_conf: dict,
    audio_conf_list: List[dict],
    video_params: dict,
    font_paths: dict,
    progress_callback=None
) -> Optional[str]:
    """
    Generate video for selected rows.
    audio_conf_list: for each of 4 segments per row, dict with fields:
        {'lang': 'EN','category': 'è‹±æ–‡ç”·å£°','voice': 'name','speed':1.0,'pause':0.5}
    video_params: {'resolution':(w,h),'fps':int,'duration_per_segment':float}
    """
    tmp_root = tempfile.mkdtemp(prefix="tts_video_")
    try:
        W,H = video_params['resolution']
        fps = video_params.get('fps', 12)
        seg_dur = video_params.get('duration_per_segment', 3.0)
        frames_per_segment = max(1, int(math.ceil(seg_dur * fps)))
        # aggregate audio files per row
        total_steps = len(selected_rows) * 4 + len(selected_rows) * 3 + 5  # rough steps for progress
        step = 0

        frame_files = []
        audio_files = []

        for idx_i, row_idx in enumerate(selected_rows):
            row = df.iloc[row_idx]
            en = str(row.get("è‹±è¯­",""))
            ph = str(row.get("éŸ³æ ‡",""))
            cn = str(row.get("ä¸­æ–‡",""))

            # For each of 4 audio segments, build text to speak according to audio_conf_list
            # Segment mapping example: [Segment1: en], [Segment2: en (alt)], [Segment3: zh], [Segment4: en]
            seg_audio_paths = []
            for seg_i, aconf in enumerate(audio_conf_list):
                text_to_speak = ""
                if aconf.get('content') == 'è‹±è¯­':
                    text_to_speak = en
                elif aconf.get('content') == 'éŸ³æ ‡':
                    text_to_speak = ph if ph else en
                elif aconf.get('content') == 'ä¸­æ–‡':
                    text_to_speak = cn
                else:
                    text_to_speak = en

                # create unique mp3 path
                mp3_path = os.path.join(tmp_root, f"row{row_idx}_seg{seg_i}.mp3")
                engine_pref = aconf.get('engine_pref','ç¦»çº¿ä¼˜å…ˆ')
                voice_choice = aconf.get('voice_choice')
                voice_category = aconf.get('voice_category')
                speed = float(aconf.get('speed',1.0))
                # generate (try parallel for edge voices)
                ok = generate_tts_segment(text_to_speak, voice_category, voice_choice, speed, engine_pref, mp3_path)
                if not ok:
                    # if fail, create silent audio placeholder of seg_dur length
                    create_silent_mp3(mp3_path, seg_dur)
                # add pause if set
                pause = float(aconf.get('pause',0.0))
                if pause > 0:
                    # append a silent mp3 for pause
                    pause_path = os.path.join(tmp_root, f"row{row_idx}_seg{seg_i}_pause.mp3")
                    create_silent_mp3(pause_path, pause)
                    seg_audio_paths.append(mp3_path)
                    seg_audio_paths.append(pause_path)
                else:
                    seg_audio_paths.append(mp3_path)
                step += 1
                if progress_callback:
                    progress_callback(min(1.0, step/total_steps))

            # merge segment audios into one audio for this row
            row_audio = os.path.join(tmp_root, f"row{row_idx}_audio.mp3")
            try:
                concat_audios_ffmpeg(seg_audio_paths, row_audio)
            except Exception:
                # fallback: try simple copy of first
                if seg_audio_paths:
                    shutil.copy(seg_audio_paths[0], row_audio)
            audio_files.append(row_audio)

            # render frames for this row: create frames_per_segment frames per segment but we can duplicate same frame
            # create single frame image and then duplicate
            frame_img = render_frame_image(en, ph, cn, style_conf, (W,H), font_paths)
            # Save frames as images
            frames_for_row = []
            for f_i in range(frames_per_segment * 4):  # 4 segments * frames per segment
                fname = os.path.join(tmp_root, f"row{row_idx}_frame_{f_i:04d}.png")
                frame_img.save(fname)
                frames_for_row.append(fname)
            frame_files.extend(frames_for_row)
            step += 1
            if progress_callback:
                progress_callback(min(1.0, step/total_steps))

        # Now create video from frames (ffmpeg)
        # To avoid writing huge temp videos, we create an image sequence video then add concatenated audio
        video_no_audio = os.path.join(tmp_root, "video_no_audio.mp4")
        try:
            # imageio can write video from image sequence
            images = [imageio.imread(p) for p in frame_files]
            # write as mp4
            imageio.mimsave(video_no_audio, images, fps=fps)
        except Exception as e:
            # fallback: use ffmpeg to create video from images list
            # Create list file
            list_txt = os.path.join(tmp_root, "imgs.txt")
            with open(list_txt, "w", encoding="utf-8") as f:
                for p in frame_files:
                    f.write(f"file '{p}'\n")
            cmd = ["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", list_txt, "-vsync", "vfr", "-pix_fmt", "yuv420p", "-r", str(fps), video_no_audio]
            subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        # Merge all row audios into one big audio
        final_audio = os.path.join(tmp_root, "final_audio.mp3")
        concat_audios_ffmpeg(audio_files, final_audio)
        # Combine video_no_audio + final_audio into final video
        final_video = os.path.join(tmp_root, "final_output.mp4")
        merge_video_audio(video_no_audio, final_audio, final_video)
        if progress_callback:
            progress_callback(1.0)
        return final_video
    except Exception as e:
        traceback.print_exc()
        return None
    finally:
        # Note: we don't immediately delete tmp_root so user can download; caller should clean up if needed
        pass

# FFmpeg helpers
def concat_audios_ffmpeg(audio_paths: List[str], out_mp3: str):
    if not audio_paths:
        raise ValueError("audio_paths empty")
    if not check_ffmpeg():
        raise RuntimeError("ffmpeg not found")
    # create list file
    listfile = out_mp3 + "_list.txt"
    with open(listfile, "w", encoding="utf-8") as f:
        for p in audio_paths:
            f.write(f"file '{os.path.abspath(p)}'\n")
    cmd = ["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", listfile, "-c", "copy", out_mp3]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    safe_remove(listfile)

def create_silent_mp3(out_path: str, duration_s: float):
    # Create silent wav using ffmpeg then convert to mp3
    if not check_ffmpeg():
        # fallback: write tiny file
        with open(out_path, "wb") as f:
            f.write(b"")
        return
    cmd = ["ffmpeg", "-y", "-f", "lavfi", "-i", f"anullsrc=r=44100:cl=mono", "-t", str(duration_s), "-q:a", "9", out_path]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

def merge_video_audio(video_path: str, audio_path: str, out_path: str):
    if not check_ffmpeg():
        raise RuntimeError("ffmpeg not found")
    cmd = ["ffmpeg", "-y", "-i", video_path, "-i", audio_path, "-c:v", "copy", "-c:a", "aac", "-shortest", out_path]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# ------------------------
# Streamlit UI
# ------------------------
st.set_page_config(page_title="æ—…è¡Œè‹±è¯­è§†é¢‘ç”Ÿæˆå™¨ - ç¦»çº¿ä¼˜å…ˆ", layout="wide")
st.title("ğŸ¬ æ—…è¡Œè‹±è¯­è§†é¢‘ç”Ÿæˆå™¨ï¼ˆç¦»çº¿ä¼˜å…ˆ + åœ¨çº¿å›é€€ï¼‰")

col_l, col_r = st.columns([2, 1])

with col_l:
    st.header("1. æ•°æ®ç®¡ç†")
    uploaded = st.file_uploader("ä¸Šä¼  Excel æ–‡ä»¶ï¼ˆ.xlsx / .xlsï¼Œå¿…é¡»åˆ—åï¼šè‹±è¯­ã€ä¸­æ–‡ã€éŸ³æ ‡ï¼ˆå¯é€‰ï¼‰ï¼‰", type=["xlsx","xls"])
    df = None
    df_errors = []
    if uploaded is not None:
        df, df_errors = validate_and_load_excel(uploaded)
        if df is None:
            for e in df_errors:
                st.error(e)
        else:
            st.success(f"æ–‡ä»¶è§£ææˆåŠŸï¼Œå…± {len(df)} è¡Œã€‚")
            if df_errors:
                for e in df_errors:
                    st.warning(e)

    if df is not None:
        st.subheader("å®æ—¶æ•°æ®é¢„è§ˆï¼ˆå‰ 10 è¡Œï¼‰")
        st.dataframe(df.head(10))

    st.markdown("---")
    st.header("2. è§†é¢‘æ ·å¼å®šåˆ¶")

    # Background mode
    bg_mode = st.selectbox("èƒŒæ™¯ç±»å‹", ["çº¯è‰²èƒŒæ™¯","å›¾ç‰‡èƒŒæ™¯"])
    style_conf = {}
    if bg_mode == "çº¯è‰²èƒŒæ™¯":
        bg_color = st.color_picker("é€‰æ‹©èƒŒæ™¯é¢œè‰²", "#ffffff")
        style_conf["bg_mode"] = "color"
        style_conf["bg_color"] = bg_color
        style_conf["bg_image_obj"] = None
    else:
        uploaded_bg = st.file_uploader("ä¸Šä¼ èƒŒæ™¯å›¾ç‰‡ï¼ˆJPG/PNGï¼‰", type=["jpg","jpeg","png"], key="bgimg")
        style_conf["bg_mode"] = "image"
        style_conf["bg_color"] = "#ffffff"
        if uploaded_bg:
            try:
                bg_img = Image.open(uploaded_bg)
                style_conf["bg_image_obj"] = bg_img.copy()
                st.image(bg_img, caption="èƒŒæ™¯é¢„è§ˆ", use_column_width=True)
            except Exception:
                st.error("èƒŒæ™¯å›¾ç‰‡è¯»å–å¤±è´¥")
                style_conf["bg_image_obj"] = None
        else:
            style_conf["bg_image_obj"] = None

    st.subheader("æ–‡å­—æ ·å¼ç³»ç»Ÿï¼ˆè‹±è¯­ / éŸ³æ ‡ / ä¸­æ–‡ï¼‰")
    # fonts (we allow custom upload of font files for phonetic if user wants)
    main_font_size = st.slider("è‹±è¯­å­—å·", 40, 160, 80)
    en_color = st.color_picker("è‹±è¯­é¢œè‰²", "#000000")
    en_bold = st.checkbox("è‹±è¯­åŠ ç²—", value=False)
    phonetic_font_path = None
    phonetic_font_file = st.file_uploader("ä¸Šä¼ éŸ³æ ‡ä¸“ç”¨å­—ä½“ï¼ˆå¯é€‰ .ttfï¼‰", type=["ttf","otf"], key="phonetic_font")
    if phonetic_font_file:
        # save to temp
        fp = os.path.join(tempfile.gettempdir(), f"phonetic_{int(time.time())}.ttf")
        with open(fp, "wb") as f:
            f.write(phonetic_font_file.read())
        phonetic_font_path = fp

    phonetic_size = st.slider("éŸ³æ ‡å­—å·", 24, 120, 56)
    ph_color = st.color_picker("éŸ³æ ‡é¢œè‰²", "#666666")
    chinese_size = st.slider("ä¸­æ–‡å­—å·", 24, 140, 68)
    cn_color = st.color_picker("ä¸­æ–‡é¢œè‰²", "#222222")
    cn_bold = st.checkbox("ä¸­æ–‡åŠ ç²—", value=False)

    # text background plate
    st.subheader("æ–‡å­—èƒŒæ™¯æ¿ï¼ˆå¯é€‰ï¼‰")
    text_bg_enable = st.checkbox("å¯ç”¨æ–‡å­—èƒŒæ™¯æ¿", value=False)
    text_bg_color = st.color_picker("èƒŒæ™¯æ¿é¢œè‰²", "#000000")
    text_bg_alpha = st.slider("èƒŒæ™¯æ¿é€æ˜åº¦", 0.0, 1.0, 0.35)
    text_bg_radius = st.slider("èƒŒæ™¯æ¿åœ†è§’", 0, 100, 12)
    text_bg_padding = st.slider("æ–‡å­—èƒŒæ™¯æ¿å†…è¾¹è·", 0, 200, 20)
    text_area_width_ratio = st.slider("æ–‡æœ¬åŒºåŸŸå®½åº¦æ¯”ä¾‹", 0.3, 1.0, 0.85)

    # spacing
    st.subheader("é—´è·ä¸æ¢è¡Œ")
    english_phonetic_gap = st.slider("è‹±è¯­ - éŸ³æ ‡ é—´è·(px)", 0, 200, 10)
    phonetic_cn_gap = st.slider("éŸ³æ ‡ - ä¸­æ–‡ é—´è·(px)", 0, 200, 10)
    line_spacing = st.slider("è¡Œé—´è·(px)", 0, 50, 6)

    # Packing style_conf
    style_conf.update({
        "english_size": main_font_size,
        "english_color": en_color,
        "english_bold": en_bold,
        "phonetic_size": phonetic_size,
        "phonetic_color": ph_color,
        "phonetic_font_path": phonetic_font_path,
        "chinese_size": chinese_size,
        "chinese_color": cn_color,
        "chinese_bold": cn_bold,
        "text_bg_enable": text_bg_enable,
        "text_bg_color": text_bg_color,
        "text_bg_alpha": text_bg_alpha,
        "text_bg_radius": text_bg_radius,
        "text_padding": text_bg_padding,
        "text_area_width_ratio": text_area_width_ratio,
        "english_phonetic_gap": english_phonetic_gap,
        "phonetic_cn_gap": phonetic_cn_gap,
        "line_spacing": line_spacing,
        "bg_mode": "image" if bg_mode == "å›¾ç‰‡èƒŒæ™¯" else "color"
    })

with col_r:
    st.header("3. å¤šéŸ³è‰²éŸ³é¢‘ç³»ç»Ÿ")
    st.markdown("æ¯æ¡æ•°æ®æ”¯æŒ 4 æ®µéŸ³é¢‘ä¸²è”ï¼ˆå¯æ··åˆä¸åŒéŸ³è‰²ï¼‰ã€‚")
    engine_pref = st.selectbox("å¼•æ“åå¥½", ["ç¦»çº¿ä¼˜å…ˆ", "åœ¨çº¿ä¼˜å…ˆ"])
    st.write("è¯­éŸ³åº“ï¼ˆé€‰æ‹©ç¤ºä¾‹éŸ³è‰²æˆ–ä½¿ç”¨ç³»ç»Ÿæœ¬åœ°éŸ³è‰²ï¼‰")
    local_voices = list_local_voices()
    local_voice_names = [v["name"] for v in local_voices] if local_voices else []
    st.write(f"ç³»ç»Ÿæœ¬åœ°è¯­éŸ³æ•°é‡: {len(local_voice_names)}")
    # Build UI for 4 segments
    audio_segments = []
    for seg_i in range(4):
        st.markdown(f"**æ®µ {seg_i+1} è®¾ç½®**")
        col_a, col_b = st.columns([1,1])
        with col_a:
            content = st.selectbox(f"æ®µ{seg_i+1} å†…å®¹", ["è‹±è¯­","éŸ³æ ‡","ä¸­æ–‡"], key=f"content_{seg_i}")
        with col_b:
            category = st.selectbox(f"æ®µ{seg_i+1} éŸ³è‰²åº“", ["è‹±æ–‡ç”·å£°","è‹±æ–‡å¥³å£°","ä¸­æ–‡éŸ³è‰²","ç³»ç»Ÿæœ¬åœ°"], index=0, key=f"cat_{seg_i}")
        voice_choice = None
        voice_category = category
        if category == "ç³»ç»Ÿæœ¬åœ°":
            if local_voice_names:
                voice_choice = st.selectbox(f"æ®µ{seg_i+1} æœ¬åœ°è¯­éŸ³é€‰æ‹©", ["(é»˜è®¤)"]+local_voice_names, key=f"localvoice_{seg_i}")
                if voice_choice != "(é»˜è®¤)":
                    # map to actual id if possible
                    for v in local_voices:
                        if v["name"] == voice_choice:
                            voice_choice = v.get("id") or v.get("name")
                            break
            else:
                st.info("æœªæ£€æµ‹åˆ°æœ¬åœ°è¯­éŸ³ï¼Œç³»ç»Ÿå°†å›é€€è‡³åœ¨çº¿è¯­éŸ³ã€‚")
                voice_choice = None
                voice_category = "è‹±æ–‡å¥³å£°"
        else:
            # present presets
            presets = VOICE_LIBRARY.get(category, [])
            if presets:
                voice_choice = st.selectbox(f"æ®µ{seg_i+1} å…·ä½“éŸ³è‰²", ["(é»˜è®¤)"] + presets, key=f"preset_{seg_i}")
                if voice_choice == "(é»˜è®¤)":
                    voice_choice = None

        speed = st.slider(f"æ®µ{seg_i+1} è¯­é€Ÿ (0.5x-2.0x)", 0.5, 2.0, 1.0, 0.1, key=f"speed_{seg_i}")
        pause = st.slider(f"æ®µ{seg_i+1} åœé¡¿ (ç§’)", 0.0, 3.0, 0.3, 0.1, key=f"pause_{seg_i}")
        audio_segments.append({
            "content": content,
            "voice_category": voice_category,
            "voice_choice": voice_choice,
            "speed": speed,
            "pause": pause,
            "engine_pref": engine_pref
        })

    st.markdown("---")
    st.subheader("è¯•å¬åŠŸèƒ½")
    # allow preview of each segment with sample text
    sample_text = st.text_input("è¯•å¬ç¤ºä¾‹æ–‡æœ¬ï¼ˆè‹¥ç©ºåˆ™ä½¿ç”¨è¡Œæ–‡æœ¬ï¼‰", value="Hello, this is a sample.")
    seg_preview_col = st.columns(4)
    for i in range(4):
        if seg_preview_col[i].button(f"è¯•å¬æ®µ {i+1}", key=f"preview_{i}"):
            conf = audio_segments[i]
            # use sample_text
            tmp_mp3 = os.path.join(tempfile.gettempdir(), f"preview_seg_{i}_{int(time.time())}.mp3")
            ok = generate_tts_segment(sample_text, conf['voice_category'], conf['voice_choice'], conf['speed'], conf['engine_pref'], tmp_mp3)
            if ok and os.path.exists(tmp_mp3):
                audio_bytes = open(tmp_mp3, "rb").read()
                st.audio(audio_bytes, format="audio/mp3")
                safe_remove(tmp_mp3)
            else:
                st.error("è¯•å¬å¤±è´¥ï¼šè¯·ç¡®è®¤ç½‘ç»œ/æœ¬åœ°è¯­éŸ³æ˜¯å¦å¯ç”¨ï¼Œæˆ–åˆ‡æ¢å¼•æ“åå¥½ã€‚")

    st.markdown("---")
    st.subheader("4. è§†é¢‘å‚æ•°é…ç½®")
    res_choice = st.selectbox("åˆ†è¾¨ç‡", list(DEFAULT_RESOLUTIONS.keys()), index=3)
    resolution = DEFAULT_RESOLUTIONS[res_choice]
    fps = st.slider("å¸§ç‡ (fps)", 8, 30, 12)
    duration_per_segment = st.slider("æ¯æ®µæ—¶é•¿ï¼ˆç§’ï¼‰", 2.0, 8.0, 3.0, 0.5)
    st.markdown("è¿›é˜¶é€‰é¡¹")
    max_rows = st.number_input("æœ€å¤šç”Ÿæˆè¡Œæ•°ï¼ˆä¸ºæ€§èƒ½ä¿å®ˆï¼Œå»ºè®® <= 50ï¼‰", min_value=1, max_value=500, value=MAX_ROWS_SUGGEST)

# bottom area for Preview & Generate
st.markdown("---")
st.header("5. é¢„è§ˆä¸ç”Ÿæˆ")

if df is None:
    st.info("è¯·å…ˆä¸Šä¼ å¹¶éªŒè¯ Excel æ•°æ®ï¼Œç„¶ååœ¨å³ä¾§è®¾ç½®æ ·å¼ä¸éŸ³é¢‘å‚æ•°ã€‚")
else:
    # Row selection for preview / generation
    total_rows = len(df)
    chosen_rows = st.multiselect("é€‰æ‹©ç”¨äºç”Ÿæˆçš„è§†é¢‘è¡Œï¼ˆæ”¯æŒå¤šé€‰ï¼›ç”Ÿæˆé¡ºåºå³é€‰æ‹©é¡ºåºï¼‰",
                                 options=list(range(total_rows)),
                                 format_func=lambda x: f"ç¬¬ {x+1} è¡Œ: {str(df.iloc[x]['è‹±è¯­'])[:40]}",
                                 default=list(range(min(5, total_rows))))
    if len(chosen_rows) == 0:
        st.warning("å°šæœªé€‰æ‹©ä»»ä½•è¡Œç”¨äºç”Ÿæˆã€‚")

    # Single-frame preview
    st.subheader("å•å¸§å®æ—¶é¢„è§ˆï¼ˆæ‰€è§å³æ‰€å¾—ï¼‰")
    preview_row_idx = st.selectbox("é€‰æ‹©é¢„è§ˆè¡Œï¼ˆä»…å½±å“ç”»é¢é¢„è§ˆï¼Œä¸ä¼šç”ŸæˆéŸ³é¢‘ï¼‰", options=chosen_rows if chosen_rows else [0])
    if preview_row_idx is None and chosen_rows:
        preview_row_idx = chosen_rows[0]
    preview_row = df.iloc[preview_row_idx]
    preview_img = render_frame_image(
        str(preview_row.get("è‹±è¯­","")),
        str(preview_row.get("éŸ³æ ‡","")),
        str(preview_row.get("ä¸­æ–‡","")),
        style_conf,
        DEFAULT_RESOLUTIONS[res_choice],
        {"main": None, "phonetic": style_conf.get("phonetic_font_path"), "chinese": None}
    )
    st.image(preview_img, caption="å•å¸§é¢„è§ˆ (æ‰€è§å³æ‰€å¾—)", use_column_width=True)

    # Generate button
    gen_col1, gen_col2 = st.columns([1,1])
    with gen_col1:
        if st.button("å¼€å§‹ç”Ÿæˆè§†é¢‘", type="primary"):
            if not check_ffmpeg():
                st.error("æœåŠ¡å™¨æœªå®‰è£… ffmpegï¼Œæ— æ³•ç”Ÿæˆè§†é¢‘ã€‚è¯·å…ˆå®‰è£… ffmpegã€‚")
            else:
                if len(chosen_rows) == 0:
                    st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€è¡Œè¿›è¡Œç”Ÿæˆã€‚")
                else:
                    if len(chosen_rows) > max_rows:
                        st.warning(f"é€‰æ‹©çš„è¡Œæ•° ({len(chosen_rows)}) è¶…è¿‡è®¾ç½®çš„æœ€å¤§è¡Œæ•° ({max_rows})ã€‚è¯·é™ä½ç”Ÿæˆæ•°é‡ä»¥å…è¶…æ—¶/å†…å­˜é—®é¢˜ã€‚")
                    # Run generation in blocking (long) operation, with progress
                    progress_bar = st.progress(0.0)
                    status_text = st.empty()

                    def progress_cb(p):
                        try:
                            progress_bar.progress(p)
                            status_text.text(f"ç”Ÿæˆè¿›åº¦ï¼š{int(p*100)}%")
                        except Exception:
                            pass

                    video_params = {
                        "resolution": resolution,
                        "fps": fps,
                        "duration_per_segment": duration_per_segment
                    }

                    # Run generation (synchronous)
                    status_text.text("å¼€å§‹ç”ŸæˆéŸ³é¢‘ä¸å¸§ï¼Œè¯·è€å¿ƒç­‰å¾…...")
                    tmp_video = generate_video_from_df(df, chosen_rows, style_conf, audio_segments, video_params,
                                                       {"main": None, "phonetic": style_conf.get("phonetic_font_path"), "chinese": None},
                                                       progress_callback=progress_cb)
                    if tmp_video and os.path.exists(tmp_video):
                        status_text.success("è§†é¢‘ç”Ÿæˆå®Œæˆï¼å‡†å¤‡ä¸‹è½½...")
                        with open(tmp_video, "rb") as f:
                            video_bytes = f.read()
                        st.video(video_bytes)
                        st.download_button("ğŸ“¥ ä¸‹è½½ MP4 è§†é¢‘", video_bytes, file_name="travel_english_video.mp4")
                        # cleanup
                        try:
                            safe_remove(os.path.dirname(tmp_video))
                        except Exception:
                            pass
                    else:
                        status_text.error("ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æˆ–æ£€æŸ¥ ffmpeg / TTS å¼•æ“æ˜¯å¦å¯ç”¨ã€‚")

    with gen_col2:
        if st.button("å¯¼å‡ºåˆå¹¶éŸ³é¢‘ (ä»…éŸ³é¢‘)"):
            # similar pipeline to merge audios only
            st.info("ç”Ÿæˆå¹¶åˆå¹¶éŸ³é¢‘...")
            tmpd = tempfile.mkdtemp(prefix="audio_merge_")
            audio_paths = []
            for ridx in chosen_rows:
                row = df.iloc[ridx]
                en = str(row.get("è‹±è¯­",""))
                ph = str(row.get("éŸ³æ ‡",""))
                cn = str(row.get("ä¸­æ–‡",""))
                # generate segments
                seg_paths = []
                for si, aconf in enumerate(audio_segments):
                    text = en if aconf["content"] == "è‹±è¯­" else (ph if aconf["content"]=="éŸ³æ ‡" else cn)
                    mp3p = os.path.join(tmpd, f"r{ridx}_s{si}.mp3")
                    ok = generate_tts_segment(text, aconf["voice_category"], aconf["voice_choice"], aconf["speed"], aconf["engine_pref"], mp3p)
                    if not ok:
                        create_silent_mp3(mp3p, duration_s=1.0)
                    seg_paths.append(mp3p)
                    # pause:
                    if aconf.get("pause",0)>0:
                        pausep = os.path.join(tmpd, f"r{ridx}_s{si}_pause.mp3")
                        create_silent_mp3(pausep, aconf.get("pause",0))
                        seg_paths.append(pausep)
                # merge segments for this row
                row_audio = os.path.join(tmpd, f"row{ridx}_merged.mp3")
                concat_audios_ffmpeg(seg_paths, row_audio)
                audio_paths.append(row_audio)
            final_audio = os.path.join(tmpd, "all_rows_merged.mp3")
            concat_audios_ffmpeg(audio_paths, final_audio)
            with open(final_audio, "rb") as f:
                st.download_button("ğŸ“¥ ä¸‹è½½åˆå¹¶éŸ³é¢‘ (MP3)", f, file_name="merged_audio.mp3")
            # cleanup
            safe_remove(tmpd)
            st.success("éŸ³é¢‘åˆå¹¶å¹¶æä¾›ä¸‹è½½ã€‚")

st.markdown("---")
st.info("æç¤ºï¼š\n- pyttsx3 ä¸ºç¦»çº¿ TTSï¼Œè´¨é‡éšæ“ä½œç³»ç»Ÿè€Œä¸åŒã€‚edge-tts ä¸ºåœ¨çº¿é«˜è´¨é‡å›é€€ã€‚\n- è¯·ç¡®ä¿æœåŠ¡å™¨å®‰è£… ffmpegã€‚è‹¥éƒ¨ç½²åˆ° Railway/Streamlit Cloudï¼Œè¯·åœ¨éƒ¨ç½²è®¾ç½®ä¸­å®‰è£… ffmpegã€‚")

# EOF
